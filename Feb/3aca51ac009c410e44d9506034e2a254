The complex mathematical formulas are playing a growing role in all walks of life: from detecting skin cancers to suggesting new Facebook friends, deciding who gets a job, how police resources are deployed, who gets insurance at what cost, or who is on a "no-fly" list.Algorithms are being used - experimentally - to write news articles from raw data, while Donald Trump's presidential campaign was helped by behavioural marketers who used an algorithm to locate the highest concentrations of "persuadable voters".But while such automated tools can inject a measure of objectivity into erstwhile subjective decisions, fears are rising over the lack of transparency that using algorithms can entail, with pressure growing to apply standards of ethics or "accountability".How algorithms (secretly) run the worldData scientist Cathy O'Neil cautioned about "blindly trusting" formulas to determine a fair outcome."Algorithms are not inherently fair, because the person who builds the model defines success," she said.O'Neil argued that while some algorithms might be helpful, others could be nefarious. In her 2016 book, Weapons of Math Destruction, she cites troubling examples in the US: Public schools in Washington DC in 2010 fired more than 200 teachers - including several well-respected ones - based on scores in an algorithmic formula that evaluated performance; A man diagnosed with bipolar disorder was rejected for employment at seven major retailers after a third-party "personality" test deemed him a high risk based on its algorithmic classification; Many jurisdictions are using "predictive policing" to shift resources to likely "hot spots". O'Neill said that depending on how data were fed into the system, this could lead to the discovery of more minor crimes and a "feedback loop" that stigmatised poor communities; Some courts rely on computer-ranked formulas to determine jail sentences and parole, which may discriminate against minorities by taking into account "risk" factors such as their neighbourhoods, and friend or family links to crime; and In the world of finance, brokers "scrape" data from online and other sources in new ways to make decisions on credit or insurance. This too often amplified prejudice against the disadvantaged, O'Neil argued.Her findings were echoed in a White House report last year warning that algorithmic systems "are not infallible - they rely on the imperfect inputs, logic, probability, and people who design them". The report said data systems ideally helped weed out human bias, but it warned against algorithms "systematically disadvantaging certain groups". Do the maths: weaker pupils, lower standardsZeynep Tufekci, a University of North Carolina professor who studies technology and society, said automated decisions were often based on data collected about people, sometimes without their knowledge."These computational systems can infer all sorts of things about you from your digital crumbs," Tufekci said in a recent lecture."They can infer your sexual orientation, your personality traits, your political leanings. They have predictive power with high levels of accuracy." Such insights might be useful in certain contexts - such as helping medical professionals diagnose postpartum depression - but unfair in others, she said.Frank Pasquale, a University of Maryland law professor and author of The Black Box Society: The Secret Algorithms That Control Money and Information, shares those concerns. He suggests one way to remedy unfair effects may be to enforce laws on consumer protection.Others caution that algorithms should not be made a scapegoat for societal ills. "People get angry and they are looking for something to blame," said Daniel Castro, vice-president of the Information Technology and Innovation Foundation. "We are concerned about bias, accountability and ethical decisions, but those exist whether you are using algorithms or not."